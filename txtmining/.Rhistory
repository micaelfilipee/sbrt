txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('http*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('www.*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('pt.*?org wiki',"",txtdf$text)
txtdf$text<-gsub('^[0-9]+[a-z]+\\b',"",txtdf$text, perl = T)
txtdf$text<-gsub('^[a-z]\\b'," ",txtdf$text,  perl = T)
txtdf$text<-gsub('^[\\W]+'," ",txtdf$text, perl = T)
txtdf$text<-gsub('[0-9]+',"",txtdf$text, perl = T)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
#sbrt_sw <- c("http", "senai", "deve","Assunto","responsavel","Nome","Instituicao","acesso","?","jan","fev","mar","abr","mai","jun","jul","ago","set","nov","dez","Brasileiro", "brasileiro", "brasil", "devem","www.sbrt.ibict.br", "serviço","serviço", "brasileiro", "respostas", "técnicas", "técnico","www.respostatecnica.org.br", "pode", "ser","norma","iso", "kg", "fig", "fonte", "sbrt", "abnt", "nbr", "tecnica",'Copyright',"FIG", "Fonte","SBRT","mm","cm2", "cm")
sbrt_sw<- iconv(sbrt_sw, from = "UTF-8", to = "ASCII//TRANSLIT")
txtdf$text<-stri_replace_all_fixed(txtdf$text,sbrt_sw,"", vectorize_all = FALSE)
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf[1,2]
#importacao dos dados
dados<-"/home/micael/R_envs/text-mining/dados/sbrt_txts/dossies"
#stopwords
stop_words_sbrt<-"dados/stop_words_sbrt.txt"
#Leitura das stopwords.
sbrt_sw<-readr::read_lines(stop_words_sbrt)
txtdf<-readtext::readtext(dados, encoding = "latin1")
## limpeza
txtdf$text<-stri_replace_all_fixed(txtdf$text,txtdf$instituicao_responsavel,"", vectorize_all = FALSE)
#txtdf$text<-stri_replace_all_fixed(txtdf$text,txtdf$data,"", vectorize_all = FALSE)
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\fDOSSIÊ TÉCNICO\nTítulo',"",txtdf$text)
#txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('c Serviço Brasileiro de Respostas Técnicas',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-gsub('Acesso em: ',"",txtdf$text)
txtdf$text<-gsub('Nome do responsável',"",txtdf$text)
txtdf$text<-gsub('Nome da Instituição do responsável',"",txtdf$text)
txtdf$text<-gsub('Data de finalização',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('http*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('www.*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('pt.*?org wiki',"",txtdf$text)
txtdf$text<-gsub('^[0-9]+[a-z]+\\b',"",txtdf$text, perl = T)
txtdf$text<-gsub('^[a-z]\\b'," ",txtdf$text,  perl = T)
txtdf$text<-gsub('^[\\W]+'," ",txtdf$text, perl = T)
txtdf$text<-gsub('[0-9]+',"",txtdf$text, perl = T)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-stri_replace_all_fixed(txtdf$text,sbrt_sw,"", vectorize_all = FALSE)
txtdf[1,2]
# pacote que possibilita a limpeza dos documentos.
library(stringr)
# pacote TM
library(tm)
library(SnowballC)
# pacote que possibilita a tokenizacao
library(tidytext)
# pacote de ferramentas de manipulação de dados e geração de graficos.
library(tidyverse)
# pacote responsavel pela clusterização dos documentos
library(stm)
library(ggridges)
## importacao
dados<-"/dossies/"
txtdf<-readtext::readtext(dados, encoding = "latin1")
## limpeza
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('www.*.br',"",txtdf$text)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
# criacao de stopwords
sw_pt_tm <- tm::stopwords("pt") %>% iconv(from = "UTF-8", to = "ASCII//TRANSLIT")
sbrt_sw <- c("http", "senai", "deve","acesso", "brasil", "devem","www.sbrt.ibict.br", "serviço", "brasileiro", "respostas", "técnicas", "técnico","www.respostatecnica.org.br", "pode", "ser","norma","iso", "kg", "fig", "fonte", "sbrt", "abnt", "nbr", "tecnica")
sw_pt_tm <- c(sbrt_sw, sw_pt_tm)
#Criação de função para transformar letras maiúsculas em minusculas:
tryTolower <- function(x){
y = NA
try_error = tryCatch(tolower(x), error = function(e) e)
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
#Criação de função para a remoção de stop words, pontuação, espaços excessivos e números.
clean.corpus<-function(corpus){
corpus <- tm_map(corpus,content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords,sw_pt)
corpus <-tm_map(corpus, removePunctuation)
corpus <-tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
return(corpus)
}
#tranformação em VectorCorpus
corpus<-VCorpus(DataframeSource(txtdf))
#limpeza 2
corpus<-clean.corpus(corpus)
# pacote que possibilita a limpeza dos documentos.
library(stringr)
# pacote TM
library(tm)
library(SnowballC)
# pacote que possibilita a tokenizacao
library(tidytext)
# pacote de ferramentas de manipulação de dados e geração de graficos.
library(tidyverse)
# pacote responsavel pela clusterização dos documentos
library(stm)
library(ggridges)
## importacao
dados<-"/dossies/"
txtdf<-readtext::readtext(dados, encoding = "latin1")
## limpeza
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('www.*.br',"",txtdf$text)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
# criacao de stopwords
sw_pt_tm <- tm::stopwords("pt") %>% iconv(from = "UTF-8", to = "ASCII//TRANSLIT")
sbrt_sw <- c("http", "senai", "deve","acesso", "brasil", "devem","www.sbrt.ibict.br", "serviço", "brasileiro", "respostas", "técnicas", "técnico","www.respostatecnica.org.br", "pode", "ser","norma","iso", "kg", "fig", "fonte", "sbrt", "abnt", "nbr", "tecnica")
sw_pt_tm <- c(sbrt_sw, sw_pt_tm)
#Criação de função para transformar letras maiúsculas em minusculas:
tryTolower <- function(x){
y = NA
try_error = tryCatch(tolower(x), error = function(e) e)
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
#Criação de função para a remoção de stop words, pontuação, espaços excessivos e números.
clean.corpus<-function(corpus){
corpus <- tm_map(corpus,content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords,sw_pt_tm)
corpus <-tm_map(corpus, removePunctuation)
corpus <-tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
return(corpus)
}
#tranformação em VectorCorpus
corpus<-VCorpus(DataframeSource(txtdf))
#limpeza 2
corpus<-clean.corpus(corpus)
knitr::opts_chunk$set(
eval = FALSE,
message = FALSE,
warning = FALSE,
include = FALSE
)
dados<-"/home/micael/R_envs/text-mining/dados/sbrt_txts/dossies"
txtdf<-readtext::readtext(dados, encoding = "latin1")
titulo<-NULL
for (i in 1:length(txtdf$text)){
tx<-strsplit(x=txtdf[i,2], "\n")[[1]][2]
titulo[i]<-tx
}
txtdf$titulo<-titulo
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('www.*.br',"",txtdf$text)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
#Diretórios dos arquivos necessários
dossies_dir<-"dados/sbrt_txts/dossies_lematizados_subs"
metadados_dir<-"dados/sbrt_dossies_metadados.json"
stop_words_sbrt<-"dados/stop_words_sbrt.txt"
#Leitura das stopwords.
sw<-readr::read_lines(stop_words_sbrt)
#Leitura e pré-processamento dos documentos.
txtdf<-get_txt(dossies_dir,metadados_dir)
#Criação de um dataframe dos metadados dos documentos.
metadados<-select(txtdf, -text)
#Criação de um datraframe apenas com os documentos e seus respectivos identificadores.
txtdf<-select(txtdf, id, text)
txtdf[1,2]
txtdf[1,3]
txtdf[2,2]
#Importação dos pacotes
library(udpipe)
library(stringr)
library(stringi)
#importacao dos dados
dados<-"/home/micael/R_envs/text-mining/dados/sbrt_txts/dossies"
#stopwords
stop_words_sbrt<-"dados/stop_words_sbrt.txt"
#Leitura das stopwords.
sbrt_sw<-readr::read_lines(stop_words_sbrt)
txtdf<-readtext::readtext(dados, encoding = "latin1")
## limpeza
txtdf$text<-stri_replace_all_fixed(txtdf$text,txtdf$instituicao_responsavel,"", vectorize_all = FALSE)
#txtdf$text<-stri_replace_all_fixed(txtdf$text,txtdf$data,"", vectorize_all = FALSE)
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\fDOSSIÊ TÉCNICO\nTítulo',"",txtdf$text)
#txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('c Serviço Brasileiro de Respostas Técnicas',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-gsub('Acesso em: ',"",txtdf$text)
txtdf$text<-gsub('Nome do responsável',"",txtdf$text)
txtdf$text<-gsub('Nome da Instituição do responsável',"",txtdf$text)
txtdf$text<-gsub('Data de finalização',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('http*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('www.*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('pt.*?org wiki',"",txtdf$text)
txtdf$text<-gsub('^[0-9]+[a-z]+\\b',"",txtdf$text, perl = T)
txtdf$text<-gsub('^[a-z]\\b'," ",txtdf$text,  perl = T)
txtdf$text<-gsub('^[\\W]+'," ",txtdf$text, perl = T)
txtdf$text<-gsub('[0-9]+',"",txtdf$text, perl = T)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-stri_replace_all_fixed(txtdf$text,sbrt_sw,"", vectorize_all = FALSE)
txtdf[2,2]
txtdf[3,2]
# pacote que possibilita a limpeza dos documentos.
library(stringr)
library(stringi)
# pacote TM
library(tm)
library(SnowballC)
# pacote que possibilita a tokenizacao
library(tidytext)
# pacote de ferramentas de manipulação de dados e geração de graficos.
library(tidyverse)
library(formattable)
# pacote responsavel pela clusterização dos documentos
library(stm)
library(ggridges)
library(jsonlite)
#importacao dos dados
dados<-"/home/micael/R_envs/text-mining/dados/sbrt_txts/dossies"
txtdf<-readtext::readtext(dados, encoding = "latin1")
metadados<-read_json("/home/micael/R_envs/text-mining/dados/sbrt_dossies_metadados.json")
for(i in 1:length(metadados)){
metadados[[i]][["dossie"]]<-names(metadados[i])
categorias<-unlist(metadados[[i]][["categoria"]], use.names = F)
metadados[[i]][["categoria"]]<-NULL
metadados[[i]][["categoria"]]<-paste(categorias,collapse=",")
palavras<-unlist(metadados[[i]][["palavras_chave"]], use.names = F)
metadados[[i]][["palavras_chave"]]<-NULL
metadados[[i]][["palavras_chave"]]<-paste(palavras,collapse=",")
rm(categorias)
rm(palavras)
}
df_metadados <- data.frame(matrix(unlist(metadados), nrow=length(metadados), byrow=T))
names(df_metadados)<-names(metadados[[1]])
df_metadados$doc_id<-df_metadados$dossie
txtdf$doc_id<-sub('.txt',"",txtdf$doc_id)
txtdf<-merge(txtdf,df_metadados, by="doc_id")
## limpeza
txtdf$text<-stri_replace_all_fixed(txtdf$text,txtdf$instituicao_responsavel,"", vectorize_all = FALSE)
#txtdf$text<-stri_replace_all_fixed(txtdf$text,txtdf$data,"", vectorize_all = FALSE)
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\fDOSSIÊ TÉCNICO\nTítulo',"",txtdf$text)
#txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('c Serviço Brasileiro de Respostas Técnicas',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-gsub('Acesso em: ',"",txtdf$text)
txtdf$text<-gsub('Nome do responsável',"",txtdf$text)
txtdf$text<-gsub('Nome da Instituição do responsável',"",txtdf$text)
txtdf$text<-gsub('Data de finalização',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('http*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('www.*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('pt.*?org wiki',"",txtdf$text)
txtdf$text<-gsub('^[0-9]+[a-z]+\\b',"",txtdf$text, perl = T)
txtdf$text<-gsub('^[a-z]\\b'," ",txtdf$text,  perl = T)
txtdf$text<-gsub('^[\\W]+'," ",txtdf$text, perl = T)
txtdf$text<-gsub('[0-9]+',"",txtdf$text, perl = T)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
sbrt_sw <- c("http", "senai", "c", "s","b","d","oc","etc", "m", "figura", "or", "p", "al", "et", "l","n","g", "deve","Assunto","responsavel","Nome","Instituicao","acesso","jan","fev","mar","abr","mai","jun","jul","ago","set","nov","dez","Brasileiro", "brasileiro", "brasil", "devem","www.sbrt.ibict.br", "serviço","serviço", "brasileiro", "respostas", "técnicas", "técnico","www.respostatecnica.org.br", "pode", "ser","norma","iso", "kg", "fig", "fonte", "sbrt", "abnt", "nbr", "tecnica",'Copyright',"FIG", "Fonte","SBRT","mm","cm2", "cm")
sbrt_sw<- iconv(sbrt_sw, from = "UTF-8", to = "ASCII//TRANSLIT")
#txtdf$text<-stri_replace_all_fixed(txtdf$text,sbrt_sw,"", vectorize_all = FALSE)
sw_pt_tm <- tm::stopwords("pt") %>% iconv(from = "UTF-8", to = "ASCII//TRANSLIT")
sw_pt_tm <- c(sbrt_sw, sw_pt_tm)
#inspeção vusual da limpeza
df_palavra <- txtdf %>%
unnest_tokens(palavra, text) %>%
filter(!palavra %in% sw_pt_tm)
df_palavra %>%
count(palavra) %>%
arrange(desc(n)) %>%
head(100) %>%
formattable()
rm(df_palavra)
rm(metadados)
rm(df_metadados)
#Limpeza e stemmatização e tokenização automática:
proc <- stm::textProcessor(txtdf$text, metadata = txtdf, language = "portuguese",customstopwords = sw_pt_tm)
out <- stm::prepDocuments(proc$documents, proc$vocab, proc$meta,
lower.thresh = 10)
rm(proc)
txtdf[3,2]
txtdf$text<-stri_replace_all_fixed(txtdf$text,sbrt_sw,"", vectorize_all = FALSE)
txtdf[3,2]
proc <- stm::textProcessor(txtdf$text, metadata = txtdf, language = "portuguese",customstopwords = sw_pt_tm)
View(proc)
#Importação dos pacotes
library(udpipe)
library(stringr)
library(stringi)
#importacao dos dados
dados<-"/home/micael/R_envs/text-mining/dados/sbrt_txts/dossies"
#stopwords
stop_words_sbrt<-"dados/stop_words_sbrt.txt"
#Leitura das stopwords.
sbrt_sw<-readr::read_lines(stop_words_sbrt)
txtdf<-readtext::readtext(dados, encoding = "latin1")
## limpeza
txtdf$text<-sub('.*\nConteúdo',"",txtdf$text)
txtdf$text<-sub('.*\nCONTEÚDO',"",txtdf$text)
txtdf$text<-sub('.*\fDOSSIÊ TÉCNICO\nTítulo',"",txtdf$text)
#txtdf$text<-sub('.*\nTítulo',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br',"",txtdf$text)
txtdf$text<-gsub('[1-9][0-9]* Copyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('[1-9][0-9]*\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.respostatecnica.org.br',"", txtdf$text)
txtdf$text<-gsub('\nCopyright © Serviço Brasileiro de Respostas Técnicas - SBRT - http://www.sbrt.ibict.br\n\n[1-9][0-9]*',"", txtdf$text)
txtdf$text<-gsub('c Serviço Brasileiro de Respostas Técnicas',"", txtdf$text)
txtdf$text<-gsub('Disponível em: ',"",txtdf$text)
txtdf$text<-gsub('Acesso em: ',"",txtdf$text)
txtdf$text<-gsub('Nome do responsável',"",txtdf$text)
txtdf$text<-gsub('Nome da Instituição do responsável',"",txtdf$text)
txtdf$text<-gsub('Data de finalização',"",txtdf$text)
txtdf$text<-str_replace_all(txtdf$text, "[^[:alnum:].:,?!;]", " ")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
txtdf$text<-gsub('Copyright Serviço Brasileiro de Respostas Técnicas SBRT http: www.respostatecnica.org.br [1-9][0-9]*',"",txtdf$text)
txtdf$text<-gsub('INTRODUÇÃO',"",txtdf$text)
txtdf$text<-gsub('Introdução',"",txtdf$text)
txtdf$text<-gsub('http*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('www.*?br ',"",txtdf$text, perl = T)
txtdf$text<-gsub('pt.*?org wiki',"",txtdf$text)
txtdf$text<-gsub('^[0-9]+[a-z]+\\b',"",txtdf$text, perl = T)
txtdf$text<-gsub('^[a-z]\\b'," ",txtdf$text,  perl = T)
txtdf$text<-gsub('^[\\W]+'," ",txtdf$text, perl = T)
txtdf$text<-gsub('[0-9]+',"",txtdf$text, perl = T)
txtdf$text<- iconv(txtdf$text, from = "UTF-8", to = "ASCII//TRANSLIT")
txtdf$text<-gsub("\\s+", " ", str_trim(txtdf$text))
"vou fazer nada"%in%"a"
"vou fazer nada"%in%"fazer"
txtdf[3,2]
#Preaparação dos documentos para criação do modelo LDA.
instance_mallet<-make_instances(txtdf, stoplist_file = stop_words_sbrt)
#faz todo o pre-processamento, tratamento de dados,
#gera todos as informações necessárias para as visualizações
#e exporta um arquivo .RData para ser importado no dashboard
setwd("/home/micael/R_envs/text-mining/")
source("relatorio/base/get_funs.R")
options(java.parameters="-Xmx6g")
library(dfrtopics)
library(tidytext)
library(tidyverse)
library(mallet)
library(dendextend)
#Diretórios dos arquivos necessários
dossies_dir<-"dados/sbrt_txts/dossies_lematizados_subs"
metadados_dir<-"dados/sbrt_dossies_metadados.json"
stop_words_sbrt<-"dados/stop_words_sbrt.txt"
#Leitura das stopwords.
sw<-readr::read_lines(stop_words_sbrt)
#Leitura e pré-processamento dos documentos.
txtdf<-get_txt(dossies_dir,metadados_dir)
#Criação de um dataframe dos metadados dos documentos.
metadados<-select(txtdf, -text)
#Criação de um datraframe apenas com os documentos e seus respectivos identificadores.
txtdf<-select(txtdf, id, text)
#Preaparação dos documentos para criação do modelo LDA.
instance_mallet<-make_instances(txtdf, stoplist_file = stop_words_sbrt)
#Busca pelo número de tópicos adequado para geração do modelo LDA.
#De acordo com a quantidade de documentos foi definido 60 como numero máximo de tópicos.
best_topic<-find_n_topics(txtdf,instance_mallet,stop_words= sw,metadados, 70)
#Criação do modelo LDA de acordo com o número de tópicos encontrado no processo anterior.
best_model<-train_model(instance_mallet,n_topics = best_topic, metadata = metadados, seed = 12345)
best_model$doc_ids<-doc_ids(best_model)
#Extração dos rótulos dos tópicos.
labels_topics<-topic_labels(best_model, n=5)
#Gera dataframe de documentos por tópico.
documents_sbrt<-tidy(best_model$model, matrix = "gamma")%>%
arrange(desc(gamma))
#Insere os metadados no dataframe gerado no passo anterior e  seleciona os 100 documentos mais relevantes.
dossies_topic<-metadados %>%
dplyr::select(id, titulo, categoria, assunto)%>%
mutate(document=id)%>%
merge(documents_sbrt)%>%
select(-id)%>%
arrange(desc(gamma))%>%
group_by(topic) %>% slice(1:100)
#Gera dataframe de termos por tópico.
terms_sbrt<-tidy(best_model$model, matrix = "beta")%>%
arrange(desc(beta))
#Criação do dendrograma de documentos
dendrograma_docs<-cluster.mallet(best_model, method = "ward.D", by="doc")%>%
as.dendrogram(hang = -1, cex = 0.06)
par(mar = c(0,0,0,0))
dendrograma_docs %>%
set("branches_k_color", k = best_topic)%>%
set("labels_cex", 1) %>%
set("leaves_cex",0.2)%>%
set("hang_leaves",0.2)%>%
set("branches_lwd", 2)%>%
plot(horiz=T, axes=F)
#Criação do dendrograma de termos
dendrograma_terms<-cluster.mallet(best_model, method = "ward.D", by="term")%>%
as.dendrogram(hang = -1, cex = 0.06)
par(mar = c(0,0,0,0))
dendrograma_terms %>%
set("branches_k_color", k = best_topic)%>%
set("labels_cex", 1) %>%
set("leaves_cex",0.2)%>%
set("hang_leaves",0.1)%>%
set("branches_lwd", 2)%>%
plot(horiz=T, axes=F)
#Armazena os objetos necessários para a visualização no dashboard.
save(best_topic,dossies_topic,terms_sbrt,dendrograma_docs,dendrograma_terms,metadados, file = "relatorio/base/sbrt_vis_2.RData")
makeLDAvis(best_model$model)
makeLDAvis(best_model$model)
par(mar = c(0,0,0,0))
dendrograma_terms %>%
set("branches_k_color", k = best_topic)%>%
set("labels_cex", 1) %>%
set("leaves_cex",0.2)%>%
set("hang_leaves",0.1)%>%
set("branches_lwd", 2)%>%
plot(horiz=T, axes=F)
setwd("~/R_envs/text-mining/relatorio")
setwd("~/R_envs/text-mining/relatorio/base")
# load data in 'global' chunk so it can be shared by all users of the dashboard
library(tidytext)
library(dplyr)
library(dendextend)
#library(biclust)
load("~/R_envs/text-mining/relatorio/base/sbrt_vis_2.RData")
set.seed(1)
#Seleção dos 100 dossiês mais relevantes por tópico.
dossies_topic<-dossies_topic %>%
group_by(topic) %>% slice(1:100)
names(dossies_topic)<-c("Dossiê", "Título", "Categoria atual do SBRT", "Assunto", "topic", "Gamma")
#Seleção dos 1000 termos mais relevantes por tópico.
terms_sbrt<-terms_sbrt%>%
group_by(topic) %>% slice(1:1000)%>%
ungroup()
names(terms_sbrt)<-c("topic", "Termo", "Beta")
# load data in 'global' chunk so it can be shared by all users of the dashboard
library(tidytext)
library(dplyr)
library(dendextend)
#library(biclust)
load("~/R_envs/text-mining/relatorio/base/sbrt_vis_2.RData")
set.seed(1)
dossies_topic_10<-meta %>%
dplyr::select(doc_id, titulo, categoria, assunto)%>%
mutate(document=doc_id)%>%
merge(documents_sbrt)%>%
select(-doc_id)%>%
arrange(desc(gamma))%>%
group_by(topic) %>% slice(1:100)
terms_sbrt<-terms_sbrt%>%
group_by(topic) %>% slice(1:1000)%>%
ungroup()
